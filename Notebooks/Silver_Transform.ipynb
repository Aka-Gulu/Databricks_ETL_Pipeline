{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b1e049ca-c4e6-4045-a6f9-04b417f3f1c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import json\n",
    "\n",
    "# Configure Path\n",
    "BRONZE_PATH = \"s3a://data-files-rjx/bronze/fire_calls\"\n",
    "SILVER_PATH = \"s3a://data-files-rjx/silver/fire_calls\"\n",
    "\n",
    "# Define Standard RulesÔºàCan be set to external json filesÔºâ\n",
    "STANDARD_RULES = {\n",
    "    \"null_handling\": {\n",
    "        \"default\": \"fill_na\",  # fill_na|drop|keep\n",
    "        \"columns\": {\n",
    "            \"Call Number\": \"drop\", # Drop Critical columns that need to exclude Null value\n",
    "            \"city\": \"drop\",\n",
    "            \"box\": \"drop\",\n",
    "            \"Station Area\": \"drop\",\n",
    "            \"Zipcode of Incident\": \"drop\",\n",
    "            \"Original Priority\": \"drop\",\n",
    "            }\n",
    "    },\n",
    "    \"type_conversion\": {\n",
    "        # For Time\n",
    "        \"Call Date\": \"date\",\n",
    "        \"Watch Date\": \"date\",\n",
    "        \"Received DtTm\": \"timestamp\",\n",
    "        \"Entry DtTm\": \"timestamp\",\n",
    "        \"Dispatch DtTm\": \"timestamp\",\n",
    "        \"Response DtTm\": \"timestamp\",\n",
    "        \"On Scene DtTm\": \"timestamp\",\n",
    "        \"Transport DtTm\": \"timestamp\",\n",
    "        \"Hospital DtTm\": \"timestamp\",\n",
    "        \"Available DtTm\": \"timestamp\",\n",
    " \n",
    "        # For Numbers\n",
    "        \"Call Number\": \"integer\",\n",
    "        \"Incident Number\": \"integer\",\n",
    "        \"Zipcode of Incident\": \"integer\",\n",
    "        \"Station Area\": \"integer\",\n",
    "        \"Box\": \"integer\",\n",
    "        \"Original Priority\": \"integer\",\n",
    "        \"Priority\": \"integer\",\n",
    "        \"Final Priority\": \"integer\",\n",
    "        \"Number of Alarms\": \"integer\",\n",
    "        \"Unit sequence in call dispatch\": \"integer\",\n",
    "        \"Fire Prevention District\":\"integer\",\n",
    "        \"Supervisor District\": \"integer\",\n",
    "\n",
    "        # For Boolean\n",
    "        \"ALS Unit\": \"boolean\"\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "def clean_null_values(df, rules):\n",
    "    \"\"\"Handle Null values based on the STANDARD RULES\"\"\"\n",
    "    from pyspark.sql.functions import lit\n",
    "    \n",
    "    # 1. Handle those to be dropped\n",
    "    for col_name, action in rules[\"null_handling\"][\"columns\"].items():\n",
    "        if action == \"drop\":\n",
    "            df = df.filter(col(col_name).isNotNull())\n",
    "        elif action == \"fill_na\":\n",
    "            df = df.na.fill({col_name: \"N/A\"})\n",
    "    \n",
    "    # 2. Handle remaining columnsÔºàUsing the default rule fill_naÔºâ\n",
    "    default_action = rules[\"null_handling\"][\"default\"]\n",
    "    if default_action == \"fill_na\":\n",
    "        # Get the remaining columns\n",
    "        remaining_cols = [c for c in df.columns \n",
    "                        if c not in rules[\"null_handling\"][\"columns\"]]\n",
    "        df = df.na.fill({c: \"N/A\" for c in remaining_cols})\n",
    "    elif default_action == \"drop\":\n",
    "        # Note: Use global drop with caution, as it may cause a large amount of data loss\n",
    "        df = df.na.drop()\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import col, coalesce, to_date, to_timestamp\n",
    "\n",
    "def convert_data_types(df, rules):\n",
    "    \"\"\"ÂÖºCompatible with Spark version\"\"\"\n",
    "    for col_name, target_type in rules[\"type_conversion\"].items():\n",
    "        if target_type == \"date\":\n",
    "            df = df.withColumn(\n",
    "                col_name,\n",
    "                coalesce(\n",
    "                    to_date(col(col_name), \"yyyy-MM-dd\"),\n",
    "                    to_date(col(col_name), \"MM/dd/yyyy\")\n",
    "                )\n",
    "            )\n",
    "        elif target_type == \"timestamp\":\n",
    "            df = df.withColumn(\n",
    "                col_name,\n",
    "                coalesce(\n",
    "                    to_timestamp(col(col_name), \"MM/dd/yyyy hh:mm:ss a\"),\n",
    "                    to_timestamp(col(col_name), \"yyyy-MM-dd HH:mm:ss\")\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            # Use cast + exception handling\n",
    "            df = df.withColumn(\n",
    "                col_name,\n",
    "                col(col_name).cast(target_type).alias(col_name))\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def process_silver():\n",
    "    try:\n",
    "        # 1. Reading Bronze Data\n",
    "        bronze_df = spark.read.parquet(BRONZE_PATH)\n",
    "        \n",
    "        # 2. Execute Data Cleansing using STANDARD_RULES\n",
    "        silver_df = (bronze_df\n",
    "            .transform(lambda df: clean_null_values(df, STANDARD_RULES))\n",
    "            .transform(lambda df: convert_data_types(df, STANDARD_RULES))\n",
    "            )\n",
    "        \n",
    "        \n",
    "        # 3. Write into Delta TableÔºà‰øùÁïôSchemaÊºîÂåñËÉΩÂäõÔºâ\n",
    "        # Using Column MappingÔºàÂÖÅËÆ∏ÁâπÊÆäÂ≠óÁ¨¶Ôºâ\n",
    "        (silver_df.write\n",
    "        .mode(\"overwrite\")\n",
    "        .format(\"delta\")\n",
    "        .option(\"delta.columnMapping.mode\", \"name\")  \n",
    "        .option(\"delta.minReaderVersion\", \"2\")\n",
    "        .option(\"delta.minWriterVersion\", \"5\")\n",
    "        .save(SILVER_PATH))\n",
    "        \n",
    "        # 4. Record Cleaning metadata\n",
    "        meta = {\n",
    "            \"processed_time\": str(current_timestamp()),\n",
    "            \"input_rows\": bronze_df.count(),\n",
    "            \"output_rows\": silver_df.count(),\n",
    "            \"rules_applied\": STANDARD_RULES\n",
    "        }\n",
    "        dbutils.fs.put(f\"{SILVER_PATH}/_processing_meta.json\", json.dumps(meta))\n",
    "        \n",
    "        print(\"‚úÖ SilverÂ§ÑÁêÜÂÆåÊàêÔºÅ\")\n",
    "        print(\"üëâ Ê∏ÖÊ¥óÂêéSchema:\")\n",
    "        silver_df.printSchema()\n",
    "        \n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Â§ÑÁêÜÂ§±Ë¥•: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "process_silver()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Silver_Transform",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
