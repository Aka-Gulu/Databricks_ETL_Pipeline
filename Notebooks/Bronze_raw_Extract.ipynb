{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3fc48946-8ec9-45c7-b230-04646f8b2580",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "import os\n",
    "from pyspark.sql.functions import current_timestamp, input_file_name\n",
    "\n",
    "# Configurations\n",
    "S3_BUCKET = \"s3a://data-files-rjx\" #USE YOUR OWN MOUNTED BUCKET\n",
    "KAGGLE_DATASET = \"imankity/san-francisco-fire-department-public-dataset\"\n",
    "\n",
    "# Initialize\n",
    "raw_path = f\"{S3_BUCKET}/raw\"\n",
    "bronze_path = f\"{S3_BUCKET}/bronze/fire_calls\"\n",
    "\n",
    "def download_from_kaggle():\n",
    "    \"\"\"Download data to S3 raw\"\"\"\n",
    "    try:\n",
    "        # Set up temp directory\n",
    "        temp_dir = \"/tmp/kaggle_download\"\n",
    "        os.makedirs(temp_dir, exist_ok=True)\n",
    "        \n",
    "        # Kaggle Authentication\n",
    "        api = KaggleApi()\n",
    "        api.authenticate()  \n",
    "        \n",
    "        # Download and unzip\n",
    "        api.dataset_download_files(\n",
    "            KAGGLE_DATASET,\n",
    "            path=temp_dir,\n",
    "            unzip=True,\n",
    "            force=True\n",
    "        )\n",
    "        \n",
    "        # Upload to S3\n",
    "        for f in os.listdir(temp_dir):\n",
    "            if f.endswith('.csv'):\n",
    "                dbutils.fs.mv(f\"file:{temp_dir}/{f}\", f\"{raw_path}/{f}\")\n",
    "        \n",
    "        print(f\"✅ 数据下载完成: {dbutils.fs.ls(raw_path)}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 下载失败: {e}\")\n",
    "        raise\n",
    "\n",
    "def raw_to_bronze():\n",
    "    \"\"\"转换CSV为Bronze层\"\"\"\n",
    "    try:\n",
    "        csv_path = next(f.path for f in dbutils.fs.ls(raw_path) if f.name.endswith('.csv'))\n",
    "        \n",
    "        (spark.read.csv(csv_path, header=True)\n",
    "            .withColumn(\"_ingest_time\", current_timestamp())\n",
    "            .withColumn(\"_source_file\", input_file_name())\n",
    "            .write.mode(\"overwrite\")\n",
    "            .parquet(bronze_path))\n",
    "        \n",
    "        print(f\"✅ Bronze层生成: {dbutils.fs.ls(bronze_path)}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 转换失败: {e}\")\n",
    "        raise\n",
    "\n",
    "# Execute\n",
    "dbutils.fs.mkdirs(raw_path)\n",
    "download_from_kaggle() and raw_to_bronze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6522c3fb-18a9-4472-8d01-43325c5454ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read data from Bronze-layer\n",
    "bronze_df = spark.read.parquet(f\"{bronze_path}\")\n",
    "\n",
    "# A quick preview\n",
    "display(bronze_df.limit(100))\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Bronze_raw_Extract",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
